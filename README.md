# ğŸ§ Spotify Skip Prediction using Logistic Regression

## ğŸ“Œ Project Overview

This project uses **Logistic Regression** to predict whether a user will skip a song within the first 30 seconds, based on audio features such as danceability, energy, loudness, tempo, and more.

The dataset was synthetically generated to simulate real-world music streaming behavior, with a binary target variable:

* `1` â†’ Skipped
* `0` â†’ Not Skipped

---

## ğŸ§  Problem Statement

With millions of tracks streamed daily, understanding **user skip behavior** is key to enhancing recommendation systems and listener experience.

> Goal: **Classify songs as "skipped" or "not skipped"** using audio features.

---

## ğŸ“‚ Dataset

* **Samples:** 1000
* **Features:**

  * `danceability`, `energy`, `loudness`, `speechiness`, `acousticness`, `instrumentalness`, `valence`, `tempo`
* **Target:** `skip_30s` (0 = Not Skipped, 1 = Skipped)

ğŸ“ Dataset generated using NumPy & Pandas to reflect realistic ranges.

---

## ğŸ§ª Approach

1. **Data Exploration**: Class balance, correlations, distributions
2. **Preprocessing**: Feature scaling with `StandardScaler`
3. **Model Training**: `LogisticRegression` from Scikit-Learn
4. **Evaluation**:

   * Accuracy
   * Confusion Matrix
   * Precision, Recall, F1-score

---

## ğŸ“ˆ Results

* **Accuracy:** \~88% (may vary)
* **F1-score for skipped songs:** Indicates model's ability to detect actual skips despite class imbalance
* **Confusion Matrix**: Gives insight into model strengths & weaknesses

---

## ğŸ’¡ Learnings

* Logistic Regression performs surprisingly well on binary classification with properly scaled features.
* Accuracy alone is **not** sufficient on imbalanced datasets â€” precision, recall, and F1-score provide deeper insights.
* Feature importance via model coefficients can offer interpretability.

---

## ğŸ› ï¸ Tech Stack

* Python
* Pandas, NumPy, Seaborn, Matplotlib
* Scikit-Learn (for model and metrics)

---

## ğŸš€ How to Run

1. Clone the repository
2. Install dependencies
3. Run `main.py` or `notebook.ipynb`
4. View outputs and plots

---

## ğŸ“š Future Improvements

* Try other models: Decision Trees, Random Forest, XGBoost
* Handle imbalance using SMOTE or class weights
* Use a real-world dataset (e.g., from Spotifyâ€™s MSSD)
* Add hyperparameter tuning

---
